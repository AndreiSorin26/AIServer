{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed, RepeatVector, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_syntax = pd.read_csv('queries_db_all.csv')\n",
    "\n",
    "tokenizer_nl = Tokenizer(filters='')\n",
    "tokenizer_sql = Tokenizer(filters='')\n",
    "\n",
    "tokenizer_nl.fit_on_texts(sql_syntax['nl'])\n",
    "tokenizer_sql.fit_on_texts(sql_syntax['syntax'])\n",
    "\n",
    "nl_sequences = tokenizer_nl.texts_to_sequences(sql_syntax['nl'])\n",
    "sql_sequences = tokenizer_sql.texts_to_sequences(sql_syntax['syntax'])\n",
    "\n",
    "max_nl_length = max(len(seq) for seq in nl_sequences)\n",
    "max_sql_length = max(len(seq) for seq in sql_sequences)\n",
    "\n",
    "nl_padded = pad_sequences(nl_sequences, maxlen=max_nl_length, padding='post')\n",
    "sql_padded = pad_sequences(sql_sequences, maxlen=max_sql_length, padding='post')\n",
    "\n",
    "X = nl_padded\n",
    "y = sql_padded\n",
    "\n",
    "sql_vocab_size = len(tokenizer_sql.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer_nl.word_index) + 1, output_dim=128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=False)))\n",
    "model.add(RepeatVector(max_sql_length))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(sql_vocab_size, activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.expand_dims(y, axis=-1)\n",
    "\n",
    "model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('syntaxer.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Input: how many users exists?\n",
      "Predicted SQL: select count(distinct [column]) [table] where [column] [column] [const] and [column] = [const];\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    decoded_sentence = ''\n",
    "    for idx in input_seq:\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        word = tokenizer_sql.index_word.get(idx, '')\n",
    "        decoded_sentence += word + ' '\n",
    "    return decoded_sentence.strip()\n",
    "\n",
    "# test_df = pd.read_csv('queries_db_all.csv')['nl']\n",
    "# test_sentences = test_df.sample(5).values\n",
    "\n",
    "\n",
    "test_sentence = \"how many users?\"\n",
    "test_sequence = tokenizer_nl.texts_to_sequences([test_sentence])\n",
    "test_padded = pad_sequences(test_sequence, maxlen=max_nl_length, padding='post')\n",
    "predicted_sequence = model.predict(test_padded)\n",
    "predicted_sentence = decode_sequence(np.argmax(predicted_sequence, axis=-1)[0])\n",
    "\n",
    "print(f\"Input: {test_sentence}\")\n",
    "print(f\"Predicted SQL: {predicted_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('queries_db_all.csv')\n",
    "df = df.head(1000)\n",
    "\n",
    "# Prepare the dataset for transformers\n",
    "dataset = Dataset.from_pandas(df[['nl', 'syntax']])\n",
    "\n",
    "# Define the tokenizer and model\n",
    "model_checkpoint = \"t5-small\"  # You can use other models like \"t5-base\" or \"t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['nl']\n",
    "    targets = examples['syntax']\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True).input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# Define the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation Results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "# Load the BLEU metric\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "# Generate predictions\n",
    "def generate_predictions(dataset, model, tokenizer):\n",
    "    inputs = tokenizer(dataset['nl'], return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs.input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
    "    predictions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return predictions\n",
    "\n",
    "predictions = generate_predictions(test_dataset, model, tokenizer)\n",
    "\n",
    "# Compute the BLEU score\n",
    "references = [[syntax] for syntax in test_dataset['syntax']]\n",
    "bleu_score = metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
